{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a05023f9",
   "metadata": {},
   "source": [
    "# Traitement et Pré-traitement de Texte (NLP)\n",
    "\n",
    "Ce notebook résume les techniques fondamentales de traitement du langage naturel (NLP) : la tokenisation, la gestion des stop words, la normalisation, ainsi que la lemmatisation et le stemming.\n",
    "\n",
    "## 1. Tokenization\n",
    "\n",
    "La tokenisation consiste à diviser une chaîne de caractères ou un texte en unités plus petites appelées **tokens**. C'est une étape essentielle pour réduire la taille du texte brut et faciliter l'analyse statistique.\n",
    "\n",
    "Il existe plusieurs types de tokenisation :\n",
    "\n",
    "* **Par mots** : Divise le texte en mots individuels.\n",
    "\n",
    "* **Par phrases** : Divise un paragraphe en phrases distinctes.\n",
    "\n",
    "* **Par caractères** ou **N-gram**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979da440",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# --- 1. Tokenization par mots ---\n",
    "text_word = \"I am back on the track.\"\n",
    "token_words = word_tokenize(text_word)\n",
    "print(f\"Résultat de la tokenization par mots : {token_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25db32ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Tokenization par phrases ---\n",
    "text_sent = \"I am you. You are not me.\"\n",
    "token_sentences = sent_tokenize(text_sent)\n",
    "print(f\"Résultat de la tokenization par phrases : {token_sentences}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78b87ef",
   "metadata": {},
   "source": [
    "*Source du code : `tokenization.py`*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7bfedd",
   "metadata": {},
   "source": [
    "## 2. Stop Words (Mots Vides)\n",
    "\n",
    "Les \"Stop Words\" sont des mots courants (articles, prépositions, pronoms) qui apportent peu de valeur sémantique (ex: \"le\", \"et\", \"dans\").\n",
    "Leur suppression permet d'optimiser les ressources informatiques et de se concentrer sur les mots-clés importants, bien que cela dépende de la tâche (à éviter pour la traduction automatique par exemple).\n",
    "\n",
    "Nous allons voir trois implémentations différentes : NLTK, Spacy et Gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd317fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Méthode 1 : NLTK ---\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Your key does not open the door\"\n",
    "stop_words_set = set(stopwords.words('english')) # Ensemble de stopwords anglais\n",
    "\n",
    "tokens = word_tokenize(text.lower())\n",
    "filtered_tokens_nltk = [word for word in tokens if word not in stop_words_set]\n",
    "\n",
    "print(\"Original:\", tokens)\n",
    "print(\"Filtré avec NLTK:\", filtered_tokens_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86904fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Méthode 2 : Spacy ---\n",
    "import spacy\n",
    "# Note: nécessite 'python -m spacy download en_core_web_sm' dans le terminal\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    filtered_words_spacy = [token.text for token in doc if not token.is_stop]\n",
    "    print(\"Filtré avec Spacy:\", filtered_words_spacy)\n",
    "except OSError:\n",
    "    print(\"Le modèle Spacy 'en_core_web_sm' n'est pas installé.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8a1e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Méthode 3 : Gensim ---\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "new_filtered_text_gensim = remove_stopwords(text)\n",
    "print(\"Filtré avec Gensim:\", new_filtered_text_gensim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9248a9",
   "metadata": {},
   "source": [
    "*Source du code : `stop_words.py`*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31893512",
   "metadata": {},
   "source": [
    "## 3. Lemmatization et Stemming\n",
    "\n",
    "Ces deux techniques servent à réduire les mots à leur forme racine, mais elles fonctionnent différemment.\n",
    "\n",
    "### Stemming (Racinisation)\n",
    "C'est une méthode basée sur des règles (troncature). Elle coupe les suffixes (ex: \"-ing\", \"-ed\") de manière mécanique. Le résultat n'est pas toujours un mot valide (ex: \"studi\" pour \"studies\").\n",
    "\n",
    "* *Avantage* : Rapide, utile pour les moteurs de recherche.\n",
    "\n",
    "* *Désavantage* : Moins précis sémantiquement.\n",
    "\n",
    "### Lemmatization\n",
    "C'est une méthode basée sur la linguistique. Elle utilise un dictionnaire et l'analyse grammaticale pour retrouver le **lemme** (la forme du dictionnaire).\n",
    "\n",
    "* *Avantage* : Préserve le sens (ex: \"better\" devient \"good\").\n",
    "\n",
    "* *Désavantage* : Plus coûteux en calcul."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a91b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# --- Stemming ---\n",
    "stemmer = PorterStemmer()\n",
    "words_to_stem = [\"learning\", \"comptutationally\", \"engineering\", \"studies\"]\n",
    "stemmed_words = [stemmer.stem(word) for word in words_to_stem]\n",
    "\n",
    "print(f\"Résultats après Stemming : {stemmed_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5921aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Lemmatization ---\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# On spécifie la partie du discours (POS) pour aider le lemmatizer\n",
    "lemmas = [\n",
    "    lemmatizer.lemmatize(\"running\", pos=wordnet.VERB), # v -> run\n",
    "    lemmatizer.lemmatize(\"better\", pos=wordnet.ADJ),   # a -> good\n",
    "    lemmatizer.lemmatize(\"studies\", pos=wordnet.NOUN), # n -> study\n",
    "    lemmatizer.lemmatize(\"was\", pos=wordnet.VERB)      # v -> be\n",
    "]\n",
    "print(f\"Résultat Lemmatization : {lemmas}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cf90f7",
   "metadata": {},
   "source": [
    "*Source du code : `lem_stem.py`*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c6cbb0",
   "metadata": {},
   "source": [
    "## 4. Normalisation Complète (Pipeline)\n",
    "\n",
    "La normalisation transforme le texte en une forme canonique standardisée. Il n'y a pas de procédure universelle, mais les étapes classiques incluent :\n",
    "\n",
    "1.  Mise en minuscules (Lowercasing).\n",
    "\n",
    "2.  Suppression des chiffres et de la ponctuation (via Regex).\n",
    "\n",
    "3.  Suppression des espaces superflus.\n",
    "\n",
    "4.  Suppression des Stop Words.\n",
    "\n",
    "Voici un exemple complet sur un texte \"sale\" contenant des logs, des dates et du bruit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7198665f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Texte brut complexe\n",
    "input_text = \"      IN 2025, following the massive DATA BREACH disclosed on 07/18/2025, investigators revealed that over 1.500.000 records—including user_IDs (e.g., U-99821#A), hashed passwords (SHA256$9f2@!), phone numbers, and transactions totaling €12,008—were exfiltrated via an unsecured API endpoint (/v1/export?debug=true), triggering alerts at 02:14:33 UTC, regulatory fines under GDPR_ART.33, internal emails marked “URGENT!!!”, incident_refs=[INC-2025-LEAK-Ω], and a public statement asserting “NO SYSTEM IS 100% IMMUNE,” punctuated by logs showing latency=1.42s, retries=5/10, and access from IPs like 185.203.44.7 before containment  .\"\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# 1. Tout en minuscule\n",
    "text_processed = input_text.lower()\n",
    "\n",
    "# 2. Enlever les chiffres\n",
    "text_processed = re.sub(r'\\d+', '', text_processed)\n",
    "\n",
    "# 3. Enlever la ponctuation (garde mots et espaces)\n",
    "text_processed = re.sub(r'[^\\w\\s]', '', text_processed)\n",
    "\n",
    "# 4. Enlever les espaces superflus (début/fin)\n",
    "text_processed = text_processed.strip()\n",
    "\n",
    "# Conversion en liste pour filtrage\n",
    "lst_input_text = text_processed.split()\n",
    "print(f\"Liste intermédiaire : {lst_input_text[:10]}...\") # Affichage partiel\n",
    "\n",
    "# 5. Enlever les stopwords et reconstruction\n",
    "final_tokens = [word for word in lst_input_text if word not in stop_words]\n",
    "final_text = \" \".join(final_tokens)\n",
    "\n",
    "print(\"\\n--- Texte Final Normalisé ---\")\n",
    "print(final_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598ca915",
   "metadata": {},
   "source": [
    "*Source du code : `text_normalization.py`*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
