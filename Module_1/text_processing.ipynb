{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a05023f9",
   "metadata": {},
   "source": [
    "# Traitement et Pré-traitement de Texte (NLP)\n",
    "\n",
    "Ce notebook résume les techniques fondamentales de traitement du langage naturel (NLP) : la tokenisation, la gestion des stop words, la normalisation, ainsi que la lemmatisation et le stemming.\n",
    "\n",
    "## 1. Tokenization\n",
    "\n",
    "La tokenisation consiste à diviser une chaîne de caractères ou un texte en unités plus petites appelées **tokens**. C'est une étape essentielle pour réduire la taille du texte brut et faciliter l'analyse statistique.\n",
    "\n",
    "Il existe plusieurs types de tokenisation :\n",
    "\n",
    "* **Par mots** : Divise le texte en mots individuels.\n",
    "\n",
    "* **Par phrases** : Divise un paragraphe en phrases distinctes.\n",
    "\n",
    "* **Par caractères** ou **N-gram**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "979da440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultat de la tokenization par mots : ['I', 'am', 'back', 'on', 'the', 'track', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# --- 1. Tokenization par mots ---\n",
    "text_word = \"I am back on the track.\"\n",
    "token_words = word_tokenize(text_word)\n",
    "print(f\"Résultat de la tokenization par mots : {token_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25db32ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultat de la tokenization par phrases : ['I am you.', 'You are not me.']\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Tokenization par phrases ---\n",
    "text_sent = \"I am you. You are not me.\"\n",
    "token_sentences = sent_tokenize(text_sent)\n",
    "print(f\"Résultat de la tokenization par phrases : {token_sentences}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78b87ef",
   "metadata": {},
   "source": [
    "*Source du code : `tokenization.py`*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7bfedd",
   "metadata": {},
   "source": [
    "## 2. Stop Words (Mots Vides)\n",
    "\n",
    "Les \"Stop Words\" sont des mots courants (articles, prépositions, pronoms) qui apportent peu de valeur sémantique (ex: \"le\", \"et\", \"dans\").\n",
    "Leur suppression permet d'optimiser les ressources informatiques et de se concentrer sur les mots-clés importants, bien que cela dépende de la tâche (à éviter pour la traduction automatique par exemple).\n",
    "\n",
    "Nous allons voir trois implémentations différentes : NLTK, Spacy et Gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd317fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: ['your', 'key', 'does', 'not', 'open', 'the', 'door']\n",
      "Filtré avec NLTK: ['key', 'open', 'door']\n"
     ]
    }
   ],
   "source": [
    "# --- Méthode 1 : NLTK ---\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Your key does not open the door\"\n",
    "stop_words_set = set(stopwords.words('english'))\n",
    "\n",
    "tokens = word_tokenize(text.lower())\n",
    "filtered_tokens_nltk = [word for word in tokens if word not in stop_words_set]\n",
    "\n",
    "print(\"Original:\", tokens)\n",
    "print(\"Filtré avec NLTK:\", filtered_tokens_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86904fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtré avec Spacy: ['key', 'open', 'door']\n"
     ]
    }
   ],
   "source": [
    "# --- Méthode 2 : Spacy ---\n",
    "import spacy\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    filtered_words_spacy = [token.text for token in doc if not token.is_stop]\n",
    "    print(\"Filtré avec Spacy:\", filtered_words_spacy)\n",
    "except OSError:\n",
    "    print(\"Le modèle Spacy 'en_core_web_sm' n'est pas installé.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be8a1e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtré avec Gensim: Your key open door\n"
     ]
    }
   ],
   "source": [
    "# --- Méthode 3 : Gensim ---\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "new_filtered_text_gensim = remove_stopwords(text)\n",
    "print(\"Filtré avec Gensim:\", new_filtered_text_gensim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9248a9",
   "metadata": {},
   "source": [
    "*Source du code : `stop_words.py`*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31893512",
   "metadata": {},
   "source": [
    "## 3. Lemmatization et Stemming\n",
    "\n",
    "Ces deux techniques servent à réduire les mots à leur forme racine, mais elles fonctionnent différemment.\n",
    "\n",
    "### Stemming (Racinisation)\n",
    "C'est une méthode basée sur des règles (troncature). Elle coupe les suffixes (ex: \"-ing\", \"-ed\") de manière mécanique. Le résultat n'est pas toujours un mot valide (ex: \"studi\" pour \"studies\").\n",
    "\n",
    "* *Avantage* : Rapide, utile pour les moteurs de recherche.\n",
    "\n",
    "* *Désavantage* : Moins précis sémantiquement.\n",
    "\n",
    "### Lemmatization\n",
    "C'est une méthode basée sur la linguistique. Elle utilise un dictionnaire et l'analyse grammaticale pour retrouver le **lemme** (la forme du dictionnaire).\n",
    "\n",
    "* *Avantage* : Préserve le sens (ex: \"better\" devient \"good\").\n",
    "\n",
    "* *Désavantage* : Plus coûteux en calcul."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00a91b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultats après Stemming : ['learn', 'comptut', 'engin', 'studi']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# --- Stemming ---\n",
    "stemmer = PorterStemmer()\n",
    "words_to_stem = [\"learning\", \"comptutationally\", \"engineering\", \"studies\"]\n",
    "stemmed_words = [stemmer.stem(word) for word in words_to_stem]\n",
    "\n",
    "print(f\"Résultats après Stemming : {stemmed_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5921aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultat Lemmatization : ['run', 'good', 'study', 'be']\n"
     ]
    }
   ],
   "source": [
    "# --- Lemmatization ---\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmas = [\n",
    "    lemmatizer.lemmatize(\"running\", pos=wordnet.VERB), # v -> run\n",
    "    lemmatizer.lemmatize(\"better\", pos=wordnet.ADJ),   # a -> good\n",
    "    lemmatizer.lemmatize(\"studies\", pos=wordnet.NOUN), # n -> study\n",
    "    lemmatizer.lemmatize(\"was\", pos=wordnet.VERB)      # v -> be\n",
    "]\n",
    "print(f\"Résultat Lemmatization : {lemmas}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cf90f7",
   "metadata": {},
   "source": [
    "*Source du code : `lem_stem.py`*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c6cbb0",
   "metadata": {},
   "source": [
    "## 4. Normalisation Complète (Pipeline)\n",
    "\n",
    "La normalisation transforme le texte en une forme canonique standardisée. Il n'y a pas de procédure universelle, mais les étapes classiques incluent :\n",
    "\n",
    "1.  Mise en minuscules (Lowercasing).\n",
    "\n",
    "2.  Suppression des chiffres et de la ponctuation (via Regex).\n",
    "\n",
    "3.  Suppression des espaces superflus.\n",
    "\n",
    "4.  Suppression des Stop Words.\n",
    "\n",
    "Voici un exemple complet sur un texte \"sale\" contenant des logs, des dates et du bruit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7198665f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liste intermédiaire : ['in', 'following', 'the', 'massive', 'data', 'breach', 'disclosed', 'on', 'investigators', 'revealed']...\n",
      "\n",
      "--- Texte Final Normalisé ---\n",
      "following massive data breach disclosed investigators revealed recordsincluding user_ids eg ua hashed passwords shaf phone numbers transactions totaling exfiltrated via unsecured api endpoint vexportdebugtrue triggering alerts utc regulatory fines gdpr_art internal emails marked urgent incident_refsincleakω public statement asserting system immune punctuated logs showing latencys retries access ips like containment\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "input_text = \"      IN 2025, following the massive DATA BREACH disclosed on 07/18/2025, investigators revealed that over 1.500.000 records—including user_IDs (e.g., U-99821#A), hashed passwords (SHA256$9f2@!), phone numbers, and transactions totaling €12,008—were exfiltrated via an unsecured API endpoint (/v1/export?debug=true), triggering alerts at 02:14:33 UTC, regulatory fines under GDPR_ART.33, internal emails marked “URGENT!!!”, incident_refs=[INC-2025-LEAK-Ω], and a public statement asserting “NO SYSTEM IS 100% IMMUNE,” punctuated by logs showing latency=1.42s, retries=5/10, and access from IPs like 185.203.44.7 before containment  .\"\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "text_processed = input_text.lower()\n",
    "\n",
    "text_processed = re.sub(r'\\d+', '', text_processed)\n",
    "\n",
    "text_processed = re.sub(r'[^\\w\\s]', '', text_processed)\n",
    "\n",
    "text_processed = text_processed.strip()\n",
    "\n",
    "lst_input_text = text_processed.split()\n",
    "print(f\"Liste intermédiaire : {lst_input_text[:10]}...\")\n",
    "\n",
    "final_tokens = [word for word in lst_input_text if word not in stop_words]\n",
    "final_text = \" \".join(final_tokens)\n",
    "\n",
    "print(\"\\n--- Texte Final Normalisé ---\")\n",
    "print(final_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598ca915",
   "metadata": {},
   "source": [
    "*Source du code : `text_normalization.py`*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
