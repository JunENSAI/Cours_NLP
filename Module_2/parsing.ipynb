{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01730931",
   "metadata": {},
   "source": [
    "# Syntaxe et Analyse Syntaxique (Parsing)\n",
    "\n",
    "Ce module explore comment les machines comprennent la structure grammaticale des phrases. Nous allons passer de l'identification des mots individuels (PoS) à l'analyse des relations complexes entre eux (Parsing).\n",
    "\n",
    "**Objectifs :**\n",
    "\n",
    "1. **PoS Tagging** : Identifier la nature des mots (Nom, Verbe, etc.).\n",
    "\n",
    "2. **Chunking** : Grouper les mots en syntagmes (Groupe Nominal).\n",
    "\n",
    "3. **Dependency Parsing** : Comprendre les relations (Sujet, Objet).\n",
    "\n",
    "4. **NER** : Identifier les entités réelles (Lieux, Organisations)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d95426c",
   "metadata": {},
   "source": [
    "## 1. Part of Speech (PoS) Tagging\n",
    "\n",
    "Le PoS est le processus d'attribution d'une catégorie grammaticale à chaque mot.\n",
    "\n",
    "* **Tokenisation** : Division du texte en unités (tokens).\n",
    "\n",
    "* **Tagging** : Attribution de l'étiquette (ex: `NN` pour Nom, `VB` pour Verbe).\n",
    "\n",
    "Nous allons comparer deux bibliothèques populaires : **NLTK** et **SpaCy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6242f330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# avec nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "text = \"Every part of our speech can be decomposed by tokens\"\n",
    "words = word_tokenize(text)\n",
    "\n",
    "pos_tags = pos_tag(words)\n",
    "\n",
    "print(\"\\nresultat PoS tagging avec nltk:\")\n",
    "for word, pos_tag in pos_tags:\n",
    "    print(f\"{word}: {pos_tag}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932ff184",
   "metadata": {},
   "outputs": [],
   "source": [
    "#avec spacy\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "print(\"resultat PoS tagging avec spacy:\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text}: {token.pos_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f88df72",
   "metadata": {},
   "source": [
    "*Source code : `pos_tag.py`*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9414f2b",
   "metadata": {},
   "source": [
    "## 2. Chunking (Analyse de surface)\n",
    "\n",
    "Une fois les mots étiquetés, nous pouvons les regrouper en **syntagmes** (chunks). Cela permet d'identifier des blocs de sens, comme un \"Groupe Nominal\" (NP).\n",
    "\n",
    "Dans l'exemple ci-dessous, nous définissons une grammaire pour capturer un déterminant, suivi d'adjectifs, suivi d'un nom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15031701",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag, RegexpParser\n",
    "\n",
    "text = \"If the party was over and our time on Earth was through\"\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "tags = pos_tag(tokens)\n",
    "\n",
    "print(f\"Tags initiaux : {tags}\\n\")\n",
    "\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "\n",
    "chunk_parser = RegexpParser(grammar)\n",
    "\n",
    "tree = chunk_parser.parse(tags)\n",
    "\n",
    "print(\"Résultat du Chunking (Arbre sous forme texte) :\")\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc3772f",
   "metadata": {},
   "source": [
    "*Source code : `chunking.py`*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0f2c37",
   "metadata": {},
   "source": [
    "## 3. Analyse de Dépendance (Dependency Parsing)\n",
    "\n",
    "Contrairement au Chunking qui groupe les mots, l'analyse de dépendance relie les mots entre eux pour comprendre \"qui fait quoi\".\n",
    "\n",
    "* **Head** : Le mot principal (ex: le verbe).\n",
    "\n",
    "* **Child** : Le mot qui en dépend (ex: le sujet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a892fdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"Autonomous cars shift insurance liability toward manufacturers\"\n",
    "doc = nlp(text)\n",
    "\n",
    "print(f\"{'MOT':<15} {'RELATION':<12} {'TÊTE (HEAD)':<15}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for token in doc:\n",
    "    print(f\"{token.text:<15} {token.dep_:<12} {token.head.text:<15}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4459c263",
   "metadata": {},
   "source": [
    "*Source code : `dependency_parsing.py`*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54cd371",
   "metadata": {},
   "source": [
    "## 4. Reconnaissance d'Entités Nommées (NER)\n",
    "\n",
    "Le NER dépasse la grammaire pour toucher au sens. Il identifie et classe des objets du monde réel (Entreprises, Pays, Dates, Montants)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4691b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
    "doc = nlp(text)\n",
    "\n",
    "print(f\"{'ENTITÉ':<20} {'LABEL':<10} {'EXPLICATION'}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text:<20} {ent.label_:<10} {spacy.explain(ent.label_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329a51b0",
   "metadata": {},
   "source": [
    "*Source code : `ner.py`*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
